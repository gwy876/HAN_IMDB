{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data : imdb unzip하여 다운\n",
    "# wget https://www.kaggle.com/c/word2vec-nlp-tutorial/download/labeledTrainData.tsv\n",
    "# 이름바꾸고 unzip\n",
    "\n",
    "\n",
    "# HAN 코드 출처\n",
    "# https://www.kaggle.com/sermakarevich/hierarchical-attention-network/comments\n",
    "# https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-HATN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from nltk import tokenize\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import Embedding, RepeatVector, Permute, Multiply, Lambda, BatchNormalization, LeakyReLU\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout, LSTM, CuDNNLSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler, Callback, ModelCheckpoint\n",
    "\n",
    "from keras.optimizers import Adam, Adadelta, SGD\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from colored import fg, bg, attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LENGTH = 100\n",
    "MAX_SENTS = 15\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "data_dir = 'data/'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "data = 'data/labeledTrainData.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_Rate = 0.001\n",
    "\n",
    "# Learning_Rate_Mode = 'SGD'\n",
    "# Learning_Rate_Mode = 'Amsgrad'\n",
    "Learning_Rate_Mode = 'Adam'\n",
    "# Learning_Rate_Mode = 'Adadelta'    \n",
    "    \n",
    "    \n",
    "SEQ_LEN = 100  # magic number - length to truncate sequences of words\n",
    "lstm_dim = 100\n",
    "dense_dim = 20\n",
    "dense_dropout = 0.2\n",
    "nb_classes = 2\n",
    "epoch = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1  With all this stuff going down at the moment w...\n",
       "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2          0  The film starts with a manager (Nicholas Bell)...\n",
       "3          0  It must be assumed that those who praised this...\n",
       "4          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imdb data 불러오기\n",
    "\n",
    "data_train = pd.read_csv( data , delimiter='\\t')\n",
    "print(data_train.shape)\n",
    "data_train = data_train.drop(['id'], axis=1)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "data_train['Processed_Reviews'] = data_train.review.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>Processed_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff go moment mj ive start listen music watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>classic war world timothy hines entertain film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film start manager nicholas bell give welcome ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>must assume praise film greatest film opera ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious 80 ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>I dont know why people think this is such a ba...</td>\n",
       "      <td>dont know people think bad movie get pretty go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>This movie could have been very good, but come...</td>\n",
       "      <td>movie could good come way short cheesy special...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>I watched this video at a friend's house. I'm ...</td>\n",
       "      <td>watch video friend house im glad waste money b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>A friend of mine bought this film for £1, and ...</td>\n",
       "      <td>friend mine buy film 1 even wa grossly overpri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;This movie is full of references. ...</td>\n",
       "      <td>br br movie full reference like mad max ii wil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review  \\\n",
       "0          1  With all this stuff going down at the moment w...   \n",
       "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3          0  It must be assumed that those who praised this...   \n",
       "4          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "5          1  I dont know why people think this is such a ba...   \n",
       "6          0  This movie could have been very good, but come...   \n",
       "7          0  I watched this video at a friend's house. I'm ...   \n",
       "8          0  A friend of mine bought this film for £1, and ...   \n",
       "9          1  <br /><br />This movie is full of references. ...   \n",
       "\n",
       "                                   Processed_Reviews  \n",
       "0  stuff go moment mj ive start listen music watc...  \n",
       "1  classic war world timothy hines entertain film...  \n",
       "2  film start manager nicholas bell give welcome ...  \n",
       "3  must assume praise film greatest film opera ev...  \n",
       "4  superbly trashy wondrously unpretentious 80 ex...  \n",
       "5  dont know people think bad movie get pretty go...  \n",
       "6  movie could good come way short cheesy special...  \n",
       "7  watch video friend house im glad waste money b...  \n",
       "8  friend mine buy film 1 even wa grossly overpri...  \n",
       "9  br br movie full reference like mad max ii wil...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/go/.local/lib/python3.6/site-packages/keras_preprocessing/text.py:177: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 15, 100)\n"
     ]
    }
   ],
   "source": [
    "# def clean_str(string):\n",
    "#     \"\"\"\n",
    "#     Tokenization/string cleaning for dataset\n",
    "#     Every dataset is lower cased except\n",
    "#     \"\"\"\n",
    "#     string = re.sub(r\"\\\\\", \"\", string, re.UNICODE)\n",
    "#     string = re.sub(r\"\\'\", \"\", string, re.UNICODE)\n",
    "#     string = re.sub(r\"\\\"\", \"\", string, re.UNICODE)\n",
    "#     return string.strip().lower()\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for idx in range(data_train.review.shape[0]):\n",
    "    text = data_train.review[idx]\n",
    "#     text = text.get_text().encode('ascii', 'ignore')\n",
    "#     text = clean_str(text)\n",
    "    text = clean_text(text)   # 캐글 전처리기 사용\n",
    "    texts.append(text)\n",
    "    sentences = tokenize.sent_tokenize(text)\n",
    "    reviews.append(sentences)\n",
    "\n",
    "    labels.append(data_train.sentiment[idx])\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# text가 여러 문장으로 되어있는 review 이다. 그러므로 document지\n",
    "# 입력데이타 : [ N , 최대문장수, 최대문장길이]\n",
    "data = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 103051 unique tokens.\n",
      "Shape of data tensor: (25000, 15, 100)\n",
      "Shape of label tensor: (25000, 2, 2, 2)\n",
      "Number of positive and negative reviews in traing and validation set\n",
      "[[[10019.  9981.]\n",
      "  [ 9981. 10019.]]\n",
      "\n",
      " [[ 9981. 10019.]\n",
      "  [10019.  9981.]]]\n",
      "[[[2481. 2519.]\n",
      "  [2519. 2481.]]\n",
      "\n",
      " [[2519. 2481.]\n",
      "  [2481. 2519.]]]\n"
     ]
    }
   ],
   "source": [
    "for i, sentences in enumerate(reviews):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Total %s unique tokens.' % len(word_index))\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print('Number of positive and negative reviews in traing and validation set')\n",
    "print(y_train.sum(axis=0))\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = 'glove'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "# building Hierachical Attention network\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# 입력 : 총 단어 갯수, 103051\n",
    "# 출력 : EMBEDDING_DIM(200)\n",
    "# embedding_matrix = (103052, 200)\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "def highlight(c, power):\n",
    "    if power >= 0.5:\n",
    "        print('{}{}{}{}'.format(bg(160), attr(1), c, attr(0)), end='')\n",
    "    elif power >= 0.4:\n",
    "        print('{}{}{}{}'.format(bg(196), attr(1), c, attr(0)), end='')\n",
    "    elif power >= 0.3:\n",
    "        print('{}{}{}{}'.format(bg(210), attr(1), c, attr(0)), end='')\n",
    "    elif power >= 0.2:\n",
    "        print('{}{}{}{}'.format(bg(217), attr(1), c, attr(0)), end='')\n",
    "    elif power >= 0.1:\n",
    "        print('{}{}{}{}'.format(bg(224), attr(1), c, attr(0)), end='')\n",
    "    elif power > 0.0:\n",
    "        print('{}{}{}{}'.format(bg(225), attr(1), c, attr(0)), end='')\n",
    "    else:\n",
    "        print(c, end='')\n",
    "        \n",
    "# visualize\n",
    "def highlight_normalize(c, power, power_min, power_max):\n",
    "    # 글자, power, 그 파워의 최소값, 최대값\n",
    "    # 최대, 최소값으로 0~1 normalize\n",
    "    power = (power - power_min) / (power_max - power_min)\n",
    "    \n",
    "    if power >= 0.9:\n",
    "        print('{}{}{}{}'.format(bg(160), attr(1), c, attr(0)), end='')\n",
    "    elif power >= 0.8:\n",
    "        print('{}{}{}{}'.format(bg(196), attr(1), c, attr(0)), end='')\n",
    "    elif power >= 0.6:\n",
    "        print('{}{}{}{}'.format(bg(210), attr(1), c, attr(0)), end='')\n",
    "    elif power >= 0.4:\n",
    "        print('{}{}{}{}'.format(bg(217), attr(1), c, attr(0)), end='')\n",
    "    elif power >= 0.2:\n",
    "        print('{}{}{}{}'.format(bg(224), attr(1), c, attr(0)), end='')\n",
    "    elif power > 0.0:\n",
    "        print('{}{}{}{}'.format(bg(225), attr(1), c, attr(0)), end='')\n",
    "    else:\n",
    "        print(c, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "#     def __init__(self, attention_dim, **kwargs):\n",
    "    def __init__(self, **kwargs):\n",
    "#         self.attention_dim = attention_dim\n",
    "\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(name='Attention_Weight',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='Attention_Bias',\n",
    "                                 shape=(input_shape[-1], ),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name='Attention_Context_Vector',\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x):\n",
    "        # refer to the original paper\n",
    "        # link: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "        \n",
    "        # RNN 구조를 거쳐서 나온 hidden states (x)에 single layer perceptron (tanh activation)\n",
    "        # 적용하여 나온 벡터가 uit \n",
    "        u_it = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        \n",
    "        # uit와 uw (혹은 us) 간의 similarity를 attention으로 사용\n",
    "        # softmax를 통해 attention 값을 확률 분포로 만듬\n",
    "        a_it = K.dot(u_it, self.u)\n",
    "        a_it = K.squeeze(a_it, -1)\n",
    "        a_it = K.softmax(a_it)\n",
    "        \n",
    "        return a_it\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word-level attention 적용하기 (for sentence representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"attention_layer_1/Softmax:0\", shape=(?, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# first, build a sentence encoder\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH, ), dtype='int32')\n",
    "embedded_sentence = embedding_layer(sentence_input)\n",
    "# [?,?,200]\n",
    "bilstm_sentence = Bidirectional(LSTM(lstm_dim, return_sequences=True))(embedded_sentence)\n",
    "\n",
    "# word attention computation, [?,200]\n",
    "# word_attention = AttentionWithContext()(bilstm_sentence)\n",
    "word_attention = AttentionLayer()(bilstm_sentence)\n",
    "print(word_attention)\n",
    "\n",
    "# word attention application, [?,200, 200]\n",
    "# hidden states의 출력에 맞게 scalar를 반복해 (lstm_dim * 2) 크기로 만듬\n",
    "repeated_word_attention = RepeatVector(lstm_dim * 2)(word_attention)\n",
    "# Permute 레이어를 사용해 Multiply가 가능하도록 차원 수정, 곱하기 위하여\n",
    "# [MAX_SENTENCE_LENGTH, lstm_dim * 2] -> [lstm_dim * 2, MAX_SENTENCE_LENGTH] \n",
    "repeated_word_attention = Permute([2, 1])(repeated_word_attention)\n",
    "\n",
    "# compute sentence representation as the weighted sum of word representations\n",
    "# 곱하고\n",
    "sentence_representation = Multiply()([bilstm_sentence, repeated_word_attention])\n",
    "# weighted-sum과정, weighting 해주고 summation, 펼쳐서 수행\n",
    "sentence_representation = Lambda(lambda x: K.sum(x, axis=1))(sentence_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence-attention 적용하기 (for document representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = Model(inputs=[sentence_input], \n",
    "                         outputs=[sentence_representation])\n",
    "\n",
    "# then, build a document encoder\n",
    "document_input = Input(shape=(MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n",
    "# 입력이 document_input 이므로, sentence 입력으로 바꾸기 위하여\n",
    "embedded_document = TimeDistributed(sentence_encoder)(document_input)\n",
    "bilstm_document = Bidirectional(LSTM(lstm_dim, return_sequences=True))(embedded_document)\n",
    "\n",
    "# sentence attention computation\n",
    "sentence_attention = AttentionLayer()(bilstm_document)\n",
    "\n",
    "# sentence attention application\n",
    "repeated_sentence_attention = RepeatVector(lstm_dim * 2)(sentence_attention)\n",
    "repeated_sentence_attention = Permute([2, 1])(repeated_sentence_attention)\n",
    "\n",
    "# compute document representation as the weighted sum of sentence representations\n",
    "document_representation = Multiply()([bilstm_document, repeated_sentence_attention])\n",
    "document_representation = Lambda(lambda x: K.sum(x, axis=1))(document_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentiment classification 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, add fc layers for classification\n",
    "hidden = BatchNormalization()(document_representation)\n",
    "hidden = Dense(dense_dim)(hidden)\n",
    "hidden = LeakyReLU(alpha=0.1)(hidden)\n",
    "hidden = Dropout(dense_dropout)(hidden)\n",
    "hidden = BatchNormalization()(document_representation)\n",
    "hidden = Dense(dense_dim)(hidden)\n",
    "hidden = LeakyReLU(alpha=0.1)(hidden)\n",
    "hidden = Dropout(dense_dropout)(hidden)\n",
    "\n",
    "pred_sentiment = Dense(nb_classes, activation='softmax')(hidden)\n",
    "\n",
    "model = Model(inputs=[document_input],\n",
    "            outputs=[pred_sentiment])\n",
    "\n",
    "# compile\n",
    "if   Learning_Rate_Mode == 'SGD':\n",
    "    opt = SGD(lr=Learning_Rate, momentum=0.9, decay=1e-6)\n",
    "elif Learning_Rate_Mode == 'Amsgrad':\n",
    "    opt = Adam(lr=Learning_Rate, amsgrad=True)\n",
    "elif Learning_Rate_Mode == 'Adam':\n",
    "    opt = Adam(lr=Learning_Rate)\n",
    "elif Learning_Rate_Mode == 'Adadelta':    \n",
    "    opt = Adadelta(lr=Learning_Rate)\n",
    "    \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 200)     20610400    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 200)     240800      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_1 (AttentionLay (None, 100)          40400       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 200, 100)     0           attention_layer_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 100, 200)     0           repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 100, 200)     0           bidirectional_1[0][0]            \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 200)          0           multiply_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 20,891,600\n",
      "Trainable params: 20,891,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 15, 100)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 15, 200)      20891600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 15, 200)      240800      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_2 (AttentionLay (None, 15)           40400       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 200, 15)      0           attention_layer_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 15, 200)      0           repeat_vector_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 15, 200)      0           bidirectional_2[0][0]            \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 200)          0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 200)          800         lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           4020        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 20)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20)           0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            42          dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 21,177,662\n",
      "Trainable params: 21,177,262\n",
      "Non-trainable params: 400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_dir = 'models/'\n",
    "os.makedirs(ck_dir, exist_ok=True)\n",
    "ck_path = ck_dir + 'model.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(ck_path,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=0,\n",
    "                             save_best_only=True,\n",
    "                             mode='min')\n",
    "early_stopping = EarlyStopping(patience = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - Hierachical attention network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/go/anaconda3/envs/venv/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 172s 9ms/step - loss: 0.3875 - acc: 0.8246 - val_loss: 0.3107 - val_acc: 0.8736\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 167s 8ms/step - loss: 0.2282 - acc: 0.9105 - val_loss: 0.3073 - val_acc: 0.8816\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 166s 8ms/step - loss: 0.1327 - acc: 0.9520 - val_loss: 0.3178 - val_acc: 0.8764\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 167s 8ms/step - loss: 0.0686 - acc: 0.9760 - val_loss: 0.4548 - val_acc: 0.8582\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 167s 8ms/step - loss: 0.0326 - acc: 0.9891 - val_loss: 0.5834 - val_acc: 0.8790\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 164s 8ms/step - loss: 0.0251 - acc: 0.9905 - val_loss: 0.6798 - val_acc: 0.8718\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "\n",
    "print(\"model fitting - Hierachical attention network\")\n",
    "hist = model.fit( x_train, y_train, \n",
    "                  nb_epoch=epoch, \n",
    "                  batch_size=batch_size,\n",
    "                  shuffle=True,\n",
    "                  callbacks=[checkpoint, early_stopping],\n",
    "                  validation_data=(x_val, y_val)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. 모델 학습 과정 표시하기\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "acc_ax = loss_ax.twinx()\n",
    "\n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "\n",
    "acc_ax.plot(hist.history['acc'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')\n",
    "\n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    "\n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention 시각화하기\n",
    "HAN에서는 word-level attention과 sentence-level attention이 함께 쓰입니다. 이 attention들을 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 넛고, word_attention layer 출력, 근데 입력이 document 므로, TimeDistributed로 만들기\n",
    "word_attention_extractor = Model(inputs=[sentence_input],\n",
    "                                 outputs=[word_attention])\n",
    "\n",
    "word_attentions = TimeDistributed(word_attention_extractor)(document_input)\n",
    "\n",
    "# doc 입력으로 단어레벨, 문장레벨 어텐션 뽑기\n",
    "attention_extractor = Model(inputs=[document_input],\n",
    "                            outputs=[word_attentions, sentence_attention])\n",
    "# (N, 15, 100)\n",
    "attention_expamle = attention_extractor.predict(x_val[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence attention visualize\n",
      "\n",
      "0.41 : \u001b[48;5;160m\u001b[1mD\u001b[0m\u001b[48;5;160m\u001b[1me\u001b[0m\u001b[48;5;160m\u001b[1ml\u001b[0m\u001b[48;5;160m\u001b[1mi\u001b[0m\u001b[48;5;160m\u001b[1mc\u001b[0m\u001b[48;5;160m\u001b[1mi\u001b[0m\u001b[48;5;160m\u001b[1mo\u001b[0m\u001b[48;5;160m\u001b[1mu\u001b[0m\u001b[48;5;160m\u001b[1ms\u001b[0m\u001b[48;5;160m\u001b[1m \u001b[0m\u001b[48;5;160m\u001b[1mh\u001b[0m\u001b[48;5;160m\u001b[1me\u001b[0m\u001b[48;5;160m\u001b[1ma\u001b[0m\u001b[48;5;160m\u001b[1ml\u001b[0m\u001b[48;5;160m\u001b[1mt\u001b[0m\u001b[48;5;160m\u001b[1mh\u001b[0m\u001b[48;5;160m\u001b[1my\u001b[0m\u001b[48;5;160m\u001b[1m \u001b[0m\u001b[48;5;160m\u001b[1mf\u001b[0m\u001b[48;5;160m\u001b[1mo\u001b[0m\u001b[48;5;160m\u001b[1mo\u001b[0m\u001b[48;5;160m\u001b[1md\u001b[0m\u001b[48;5;160m\u001b[1m.\u001b[0m\n",
      "0.28 : \u001b[48;5;210m\u001b[1mT\u001b[0m\u001b[48;5;210m\u001b[1mh\u001b[0m\u001b[48;5;210m\u001b[1me\u001b[0m\u001b[48;5;210m\u001b[1m \u001b[0m\u001b[48;5;210m\u001b[1ms\u001b[0m\u001b[48;5;210m\u001b[1mt\u001b[0m\u001b[48;5;210m\u001b[1me\u001b[0m\u001b[48;5;210m\u001b[1ma\u001b[0m\u001b[48;5;210m\u001b[1mk\u001b[0m\u001b[48;5;210m\u001b[1m \u001b[0m\u001b[48;5;210m\u001b[1mi\u001b[0m\u001b[48;5;210m\u001b[1ms\u001b[0m\u001b[48;5;210m\u001b[1m \u001b[0m\u001b[48;5;210m\u001b[1ma\u001b[0m\u001b[48;5;210m\u001b[1mm\u001b[0m\u001b[48;5;210m\u001b[1ma\u001b[0m\u001b[48;5;210m\u001b[1mz\u001b[0m\u001b[48;5;210m\u001b[1mi\u001b[0m\u001b[48;5;210m\u001b[1mn\u001b[0m\u001b[48;5;210m\u001b[1mg\u001b[0m\u001b[48;5;210m\u001b[1m.\u001b[0m\n",
      "0.14 : \u001b[48;5;224m\u001b[1mF\u001b[0m\u001b[48;5;224m\u001b[1mi\u001b[0m\u001b[48;5;224m\u001b[1ms\u001b[0m\u001b[48;5;224m\u001b[1mh\u001b[0m\u001b[48;5;224m\u001b[1m \u001b[0m\u001b[48;5;224m\u001b[1ma\u001b[0m\u001b[48;5;224m\u001b[1mn\u001b[0m\u001b[48;5;224m\u001b[1md\u001b[0m\u001b[48;5;224m\u001b[1m \u001b[0m\u001b[48;5;224m\u001b[1mp\u001b[0m\u001b[48;5;224m\u001b[1mo\u001b[0m\u001b[48;5;224m\u001b[1mr\u001b[0m\u001b[48;5;224m\u001b[1mk\u001b[0m\u001b[48;5;224m\u001b[1m \u001b[0m\u001b[48;5;224m\u001b[1ma\u001b[0m\u001b[48;5;224m\u001b[1mr\u001b[0m\u001b[48;5;224m\u001b[1me\u001b[0m\u001b[48;5;224m\u001b[1m \u001b[0m\u001b[48;5;224m\u001b[1ma\u001b[0m\u001b[48;5;224m\u001b[1mw\u001b[0m\u001b[48;5;224m\u001b[1me\u001b[0m\u001b[48;5;224m\u001b[1ms\u001b[0m\u001b[48;5;224m\u001b[1mo\u001b[0m\u001b[48;5;224m\u001b[1mm\u001b[0m\u001b[48;5;224m\u001b[1me\u001b[0m\u001b[48;5;224m\u001b[1m \u001b[0m\u001b[48;5;224m\u001b[1mt\u001b[0m\u001b[48;5;224m\u001b[1mo\u001b[0m\u001b[48;5;224m\u001b[1mo\u001b[0m\u001b[48;5;224m\u001b[1m.\u001b[0m\n",
      "0.07 : \u001b[48;5;225m\u001b[1mS\u001b[0m\u001b[48;5;225m\u001b[1me\u001b[0m\u001b[48;5;225m\u001b[1mr\u001b[0m\u001b[48;5;225m\u001b[1mv\u001b[0m\u001b[48;5;225m\u001b[1mi\u001b[0m\u001b[48;5;225m\u001b[1mc\u001b[0m\u001b[48;5;225m\u001b[1me\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1mi\u001b[0m\u001b[48;5;225m\u001b[1ms\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1ma\u001b[0m\u001b[48;5;225m\u001b[1mb\u001b[0m\u001b[48;5;225m\u001b[1mo\u001b[0m\u001b[48;5;225m\u001b[1mv\u001b[0m\u001b[48;5;225m\u001b[1me\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1ma\u001b[0m\u001b[48;5;225m\u001b[1mn\u001b[0m\u001b[48;5;225m\u001b[1md\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1mb\u001b[0m\u001b[48;5;225m\u001b[1me\u001b[0m\u001b[48;5;225m\u001b[1my\u001b[0m\u001b[48;5;225m\u001b[1mo\u001b[0m\u001b[48;5;225m\u001b[1mn\u001b[0m\u001b[48;5;225m\u001b[1md\u001b[0m\u001b[48;5;225m\u001b[1m.\u001b[0m\n",
      "0.05 : \u001b[48;5;225m\u001b[1mN\u001b[0m\u001b[48;5;225m\u001b[1mo\u001b[0m\u001b[48;5;225m\u001b[1mt\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1ma\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1mb\u001b[0m\u001b[48;5;225m\u001b[1ma\u001b[0m\u001b[48;5;225m\u001b[1md\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1mt\u001b[0m\u001b[48;5;225m\u001b[1mh\u001b[0m\u001b[48;5;225m\u001b[1mi\u001b[0m\u001b[48;5;225m\u001b[1mn\u001b[0m\u001b[48;5;225m\u001b[1mg\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1mt\u001b[0m\u001b[48;5;225m\u001b[1mo\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1ms\u001b[0m\u001b[48;5;225m\u001b[1ma\u001b[0m\u001b[48;5;225m\u001b[1my\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1ma\u001b[0m\u001b[48;5;225m\u001b[1mb\u001b[0m\u001b[48;5;225m\u001b[1mo\u001b[0m\u001b[48;5;225m\u001b[1mu\u001b[0m\u001b[48;5;225m\u001b[1mt\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1mt\u001b[0m\u001b[48;5;225m\u001b[1mh\u001b[0m\u001b[48;5;225m\u001b[1mi\u001b[0m\u001b[48;5;225m\u001b[1ms\u001b[0m\u001b[48;5;225m\u001b[1m \u001b[0m\u001b[48;5;225m\u001b[1mp\u001b[0m\u001b[48;5;225m\u001b[1ml\u001b[0m\u001b[48;5;225m\u001b[1ma\u001b[0m\u001b[48;5;225m\u001b[1mc\u001b[0m\u001b[48;5;225m\u001b[1me\u001b[0m\u001b[48;5;225m\u001b[1m.\u001b[0m\n",
      "0.05 : Worth every penny!\n",
      "\n",
      "word attention visualize\n",
      "\n",
      "\u001b[48;5;160m\u001b[1mdelicious\u001b[0m \u001b[48;5;217m\u001b[1mhealthy\u001b[0m food \n",
      "\u001b[48;5;225m\u001b[1mthe\u001b[0m steak \u001b[48;5;225m\u001b[1mis\u001b[0m \u001b[48;5;160m\u001b[1mamazing\u001b[0m \n",
      "\u001b[48;5;225m\u001b[1mfish\u001b[0m \u001b[48;5;225m\u001b[1mand\u001b[0m pork \u001b[48;5;225m\u001b[1mare\u001b[0m \u001b[48;5;160m\u001b[1mawesome\u001b[0m \u001b[48;5;225m\u001b[1mtoo\u001b[0m \n",
      "\u001b[48;5;160m\u001b[1mservice\u001b[0m \u001b[48;5;224m\u001b[1mis\u001b[0m \u001b[48;5;210m\u001b[1mabove\u001b[0m and \u001b[48;5;225m\u001b[1mbeyond\u001b[0m \n",
      "\u001b[48;5;225m\u001b[1mnot\u001b[0m \u001b[48;5;225m\u001b[1ma\u001b[0m \u001b[48;5;210m\u001b[1mbad\u001b[0m \u001b[48;5;225m\u001b[1mthing\u001b[0m \u001b[48;5;225m\u001b[1mto\u001b[0m say \u001b[48;5;225m\u001b[1mabout\u001b[0m \u001b[48;5;160m\u001b[1mthis\u001b[0m \u001b[48;5;160m\u001b[1mplace\u001b[0m \n",
      "worth \u001b[48;5;160m\u001b[1mevery\u001b[0m \u001b[48;5;217m\u001b[1mpenny\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "word_rev_index = {}\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    word_rev_index[i] = word\n",
    "\n",
    "def sentiment_analysis(review): \n",
    "    sentences = tokenize.sent_tokenize(review)  # 문단->문장단위로 자르기\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)  # 벡터화\n",
    "    tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=MAX_SENT_LENGTH) # 문장 최대길이로 제로패딩, 앞쪽으로 0이 붙는다\n",
    "    pad_size = MAX_SENTS - tokenized_sentences.shape[0]  # 15 - 6 = 9, 문장단위로는 9번 패딩해야한다\n",
    "\n",
    "    # tokenized_sentences : [15 , 100]\n",
    "    if pad_size <= 0:\n",
    "        tokenized_sentences = tokenized_sentences[:MAX_SENTS]  # 최대 문장수보다 크면 MAX_SENTS 만큼 자르고\n",
    "    else:\n",
    "        tokenized_sentences = np.pad(\n",
    "            tokenized_sentences, ((0, pad_size), (0, 0)),\n",
    "            mode='constant', constant_values=0\n",
    "        )  # 그렇지 않으면 죄다 0으로 패딩\n",
    "\n",
    "    # 단어기준, 문장기준 어텐션 가져오기, np.asarray를 이용해서 list를 np array로 만들어서 너어준다\n",
    "    pred_word_attention    = attention_extractor.predict(np.asarray([tokenized_sentences]))[0]  # [0] : word attention만 가져오기, (1, 15, 100)\n",
    "    pred_sentence_attention= attention_extractor.predict(np.asarray([tokenized_sentences]))[1]  # [1] : sentence attention만 가져오기, (1, 15)\n",
    "\n",
    "    # 문장단위 어탠션 nomalize, [0.30764675, 0.27119225, 0.17760307, 0.11542551, 0.07580608, 0.05232629]\n",
    "    real_sentence_attention = pred_sentence_attention[0][:-pad_size]\n",
    "    real_sentence_attention = real_sentence_attention / (np.sum(real_sentence_attention))  # normalize\n",
    "    real_sentence_attention\n",
    "    # 문장단위 어텐션 visualize\n",
    "    print('Sentence attention visualize\\n')\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        print(round(real_sentence_attention[i],2),': ' , end='')\n",
    "        for j in sentence:\n",
    "            highlight_normalize(j, real_sentence_attention[i], np.min(real_sentence_attention), np.max(real_sentence_attention) )\n",
    "        print()\n",
    "\n",
    "    # 단어단위 어텐션 visualize\n",
    "    print('\\nword attention visualize\\n')\n",
    "    for i, sentence in enumerate(tokenized_sentences[:-pad_size]):\n",
    "        # word_rev_index로 idx -> 실제 단어, 0제외\n",
    "        words = [word_rev_index[word_id] for word_id in sentence if word_id != 0][:50]\n",
    "        pred_att = np.asarray(pred_word_attention[0][i][::-1][:len(words)][::-1])  # [::-1]로 순서 뒤집고, 단어수만큼만 짜르고, 다시 순서 뒤집어서 단어만 나오도록\n",
    "\n",
    "        for j, word in enumerate(words):\n",
    "            highlight_normalize(word, pred_att[j], np.min(pred_att), np.max(pred_att))\n",
    "            print(' ',end='')\n",
    "        print()\n",
    "# heatmap 방식으로 visualize   \n",
    "#     for i, sentence in enumerate(tokenized_sentences[:-pad_size]):\n",
    "#     # word_rev_index로 idx -> 실제 단어, 0제외\n",
    "#     words = [word_rev_index[word_id] for word_id in sentence if word_id != 0][:50]\n",
    "#     pred_att = np.asarray(pred_word_attention[0][i][::-1][:len(words)][::-1])  # [::-1]로 순서 뒤집고, 단어수만큼만 짜르고, 다시 순서 뒤집어서 단어만 나오도록\n",
    "#     pred_att = np.expand_dims(pred_att, axis=0)  # [[0.01076243 0.01147179 0.0133635  0.03013644]], array 한겹 더 씌우기\n",
    "#     fig, ax = plt.subplots(figsize=(len(words), 2))\n",
    "#     plt.rc('xtick', labelsize=15)\n",
    "#     heatmap = sn.heatmap(pred_att, xticklabels=words, square=True, linewidths=5, cmap=\"Blues\", cbar=False)\n",
    "#     plt.xticks(rotation=0)\n",
    "#     plt.show()    \n",
    "sentiment_analysis(\"Delicious healthy food. The steak is amazing. Fish and pork are awesome too. Service is above and beyond. Not a bad thing to say about this place. Worth every penny!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## !! 차원이안맞는다!! y_val 이 [2,2] 니까 확인하자\n",
    "\n",
    "# F1 score & confusion matrix 구하기 위해서 [1,0] -> [True] 식으로 변환\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "prediction = model.predict(x_val)\n",
    "y_val = y_val\n",
    "y_pred = (prediction > 0.5)\n",
    "y_val_temp = (y_val > 0.5)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i][1] > 0.5:\n",
    "        y_pred.append(True)\n",
    "    else:\n",
    "        y_pred.append(False)\n",
    "        \n",
    "y_val_temp = []\n",
    "for i in range(len(y_val)):\n",
    "    if y_val[i][1] == 1:\n",
    "        y_val_temp.append(True)\n",
    "    else:\n",
    "        y_val_temp.append(False)\n",
    "        \n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_val_temp)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_val_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-49971ffe0477>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0my_val_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0my_val_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
